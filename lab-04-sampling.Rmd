---
title: "Lab 04 — Sampling from Time Series"
author: "EAES 480 — Modern Statistics in Earth & Environmental Science"
date: "`r format(Sys.Date(), '%B %d, %Y')`"
output:
  html_document:
    theme: cosmo
    highlight: tango
    toc: true
    toc_depth: 3
    toc_float: true
    code_folding: hide
    df_print: paged
editor_options:
  chunk_output_type: inline
---

```{r setup, echo=FALSE, message=FALSE, warning=T}
library(tidyverse)
library(lubridate)
library(janitor)

knitr::opts_chunk$set(
  warning = FALSE,
  message = FALSE
)
```

# Overview

## What is AmeriFlux?

**AmeriFlux** is a network of **eddy covariance** flux-tower sites that measure exchanges of **carbon (CO₂), water, and energy** between ecosystems and the atmosphere, with standardized data products shared for research and education.

In this lab, you will use a simplified AmeriFlux-style dataset from the **US-AMS site at Argonne National Laboratory (near Chicago)**. The measurements are at **30-minute resolution** over **2023**, and show strong **seasonality** and **day–night cycles**.

**Key idea for this lab:** treat the full 2023 time series as the **population**, then practice sampling strategies to estimate population parameters.

References for context:
- AmeriFlux overview: https://ameriflux.lbl.gov/about/about-ameriflux/
- US-AMS site page: https://ameriflux.lbl.gov/sites/siteinfo/US-AMS

---

# Learning goals

By the end of this lab, you should be able to:

- Define a **population** and a **sample** for an EAES time-series dataset
- Compute population **parameters** (mean, SD) and compare to sample **estimates**
- Visualize distributions and identify **latent grouping variables** (month, day/night)
- Implement **simple random sampling** and **stratified sampling**
- Use `set.seed()` to make sampling reproducible

---

# Data

## Load and inspect

```{r load_data, echo=TRUE, eval=T}
df <- read_csv("data/us-ams-simple.csv",
               na = c("-9999")) %>%
  clean_names() %>% filter(year_local == 2023)

glimpse(df)
```

**CHECK:** You should see columns like `year_local`, `doy`, `daytime`, and flux/biomet variables (e.g., `gpp`, `fc`, `le`, `ta`).

---

## Create a date and month column from DOY

This dataset uses **Year + Day-of-Year (DOY)**. Month must be derived from a calendar date.

```{r derive_time, echo=TRUE, eval=T}
df <- df %>%
  filter(!is.na(daytime)) %>%
  mutate(
    # TODO: create a Date column from year_local and doy
    # HINT: Jan 1 is DOY = 1, so use (doy - 1) with origin = "YYYY-01-01"
    date = as.Date(doy - 1, origin = paste0(year_local, "-01-01")),

    # TODO: create a month column (numeric 1–12 or labeled months)
    month = month(date, label = TRUE, abbr = TRUE),

    # TODO: make a day/night label using daytime (0/1)
    day_night = if_else(daytime == 1, "Day", "Night")
  )

count(df, month)
count(df, day_night)
```

---

# Choose a response variable

You will analyze **one response variable** throughout the lab. This could be a CO₂ flux metric or a meteorological variable.

Examples you can choose from (depending on what you see in the dataset):
- CO₂ / carbon: `gpp`, `reco`, `fc`
- Energy: `le`
- Meteorology: `ta`, `ts`, `swc`

```{r choose_response, echo=TRUE, eval=T}
# TODO: choose ONE response variable (a column name as a string)
response_var <- "le"   # replace "gpp" with your choice, e.g. "fc" or "le" or "ta"

# CHECK: print a quick summary
df %>% summarise(
  n = n(),
  n_missing = sum(is.na(df[[response_var]])),
  mean = mean(df[[response_var]], na.rm = TRUE),
  sd = sd(df[[response_var]], na.rm = TRUE)
)
```

**Prompt (2–3 sentences):** Why did you choose this response variable? What do you expect its seasonality/day–night pattern to be?

> *Write your answer here.*

---

# Section 1 — Data dictionary (conceptual)

Students will populate the data dictionary using:
https://ameriflux.lbl.gov/data/aboutdata/data-variables/

Fill in at least **5 variables** from this dataset:

| Variable | Units | Description | Expected sign/seasonality? |
|----------|-------|-------------|----------------------------|
|     DO     |   µmol L-1    |      Dissolved oxygen in water       |         Peaks in winter, lowest in summer.                   |
|     PCH4     |    nmolCH4 mol-1   |     Dissolved methane (CH4) in water        |         Peaks in summer/early fall, lowest in winter.                   |
|    PA      |   kPa    |       Atmospheric pressure      |              Generally higher in winter, lower in summer.              |
|    D_SNOW      |     	cm  |      Snow depth       |            Peaks in winter/early spring, zero in summer.                |
|     P     |    mm   |      Precipitation       |              Highly region-dependent.              |

---

# Section 2 — Visualizing the population

Remember: for this lab, the **population** is the entire 2023 half-hourly time series.

## 2.1 Time series view

```{r plot_time_series, echo=TRUE, eval=FALSE}
# GOAL: Visualize seasonality over the year.
# TODO: pick a y aesthetic using response_var.

df %>%
  ggplot(aes(x = date, y = df[[response_var]])) +
  geom_line(alpha = 0.25) +
  theme_classic(base_size = 18) +
  labs(
    x = NULL,
    y = response_var,
    title = "Population time series (2023)"
  )
```

**Prompt (2–3 sentences):** What major patterns do you see? (Seasonal cycle? Daily cycle? Outliers?)

> *Summer shows high peaks, while winter has near 0 latent heat flux. There are also day and night cycles with no visible outliers.*

---

## 2.2 Population distribution (histogram + density)

```{r pop_distribution, echo=TRUE, eval=T}
# GOAL: See the overall distribution of the population.
# TODO: choose an appropriate number of bins (start with ~50).

ggplot(df, aes(x = df[[response_var]])) +
  geom_histogram(bins = 50, alpha = 0.7) +
  theme_classic(base_size = 18) +
  labs(x = response_var, y = "Count", title = "Population distribution (histogram)")

ggplot(df, aes(x = df[[response_var]])) +
  geom_density(alpha = 0.7) +
  theme_classic(base_size = 18) +
  labs(x = response_var, y = "Density", title = "Population distribution (density)")
```

**Prompt:** Describe shape (skew, modality), center, and spread.

> *The distribution is unimodal with a right skew, the center is around 10 with a extensive spread.*

---

## 2.3 Do latent groups explain variability? (month, day/night)

### By month

```{r pop_by_month, echo=TRUE, eval=T}
# TODO: make month appear in a sensible order (it already is an ordered factor if label=TRUE)
df %>%
  ggplot(aes(x = month, y = df[[response_var]])) +
  geom_boxplot(outlier.alpha = 0.25) +
  theme_classic(base_size = 18) +
  labs(x = NULL, y = response_var, title = "Population by month")
```

### By day/night

```{r pop_by_daynight, echo=TRUE, eval=T}
df %>%
  ggplot(aes(x = day_night, y = df[[response_var]])) +
  geom_boxplot(outlier.alpha = 0.25) +
  theme_classic(base_size = 18) +
  labs(x = NULL, y = response_var, title = "Population by day vs night")
```

**Prompt (3–4 sentences):** Which grouping variable (month or day/night) seems to explain more variability in your response? Why?

> *Day/night seems to explain more variability because latent heat flux is primarily driven by solar radiation, the nighttime values are tightly clustered near zero with very little spread, regardless of the time of year.*

---

# Section 3 — Population parameters (truth)

Compute the population mean and SD for your chosen response variable.

```{r population_params, echo=TRUE, eval=T}
pop_mean <- mean(df[[response_var]], na.rm = TRUE)
pop_sd   <- sd(df[[response_var]], na.rm = TRUE)

tibble(
  response_var = response_var,
  population_mean = pop_mean,
  population_sd = pop_sd
)
```

---

# Section 4 — Simple random sampling (SRS)

## 4.1 One random sample

```{r one_sample, echo=TRUE, eval=T}
set.seed(480)

# TODO: choose a sample size (e.g., 200, 500, 1000)
n_samp <- 1000

samp <- df %>%
  slice_sample(n = n_samp)

samp_mean <- mean(samp[[response_var]], na.rm = TRUE)
samp_sd   <- sd(samp[[response_var]], na.rm = TRUE)

tibble(
  n_samp = n_samp,
  sample_mean = samp_mean,
  sample_sd = samp_sd,
  pop_mean = pop_mean,
  pop_sd = pop_sd
)
```

**Prompt (2–3 sentences):** How close is your one-sample estimate to the population mean/SD? Is the difference surprising?

> *The one-sample estimates are very close to the true population parameters, the difference is not surprising because of the large numbers law.*

---

## 4.2 Sampling variability: many samples → many means

```{r sampling_distribution, echo=TRUE, eval=T}
set.seed(480)

reps <- 1000   # TODO: choose number of replicates (e.g., 500 or 1000)

means <- replicate(
  reps,
  df %>%
    slice_sample(n = n_samp) %>%
    summarise(m = mean(le, na.rm = TRUE)) %>%
    pull(m)
)

ggplot(tibble(mean_est = means), aes(mean_est)) +
  geom_histogram(bins = 40, alpha = 0.8) +
  geom_vline(xintercept = pop_mean, linetype = "dashed", linewidth = 1.1) +
  theme_classic(base_size = 18) +
  labs(
    x = paste0("Sample mean of ", response_var),
    y = "Count",
    title = "Sampling distribution of the mean (SRS)",
    subtitle = "Dashed line = population mean"
  )
```

**Prompt (2–3 sentences):** Is the sampling distribution centered on the population mean? What happens if you increase `n_samp`?

> *Yes, it is centered. Increasing n_samp will reduce the distribution spreading.*

---

# Section 5 — Stratified sampling

Here you’ll test whether stratification helps when the population has structure.

## 5.1 Stratify by month

```{r strat_by_month, echo=TRUE, eval=T}
set.seed(480)

# GOAL: sample within each month to ensure seasonal representation.
# TODO: choose n_per_month so total sample size is reasonable (e.g., 12 * 20 = 240)
n_per_month <- 1200

samp_strat <- df %>%
  group_by(month) %>%
  slice_sample(n = n_per_month) %>%
  ungroup()

strat_mean <- mean(samp_strat[[response_var]], na.rm = TRUE)
strat_sd   <- sd(samp_strat[[response_var]], na.rm = TRUE)

tibble(
  n_per_month = n_per_month,
  total_n = nrow(samp_strat),
  strat_mean = strat_mean,
  strat_sd = strat_sd,
  pop_mean = pop_mean,
  pop_sd = pop_sd
)
```

---

## 5.2 Compare strategies (SRS vs stratified)

```{r compare_sampling, echo=TRUE, eval=T}
tibble(
  strategy = c("Population", "SRS", "Stratified by month"),
  mean = c(pop_mean, samp_mean, strat_mean),
  sd   = c(pop_sd,   samp_sd,   strat_sd)
)
```

**Prompt (3–4 sentences):** Which strategy better approximated the population mean and SD for your response variable? Why might stratification help (or not) here?

> *Stratified by month has a better approximation compared to SRS. By stratifying by month, that forced the sample to proportionally represent every part of the year, guaranteeing that both the summer months and winter months are accurately captured.*

---

# Section 6 — Conceptual reflection

Answer in **4–6 sentences**:

- Why does seasonality matter for sampling?
- What happens if sampling ignores latent grouping variables?
- In EAES field studies, when is stratification essential?
- What is one trade-off of stratified sampling?

> *Write your answer here.*

---

# Part II — Sampling designs extensions (graded practice)

In Part I you treated the full 2023 half-hourly record as a **population**, then compared **simple random sampling** (SRS) vs **stratified sampling by month**.

Now you will practice additional **sampling designs** discussed in lecture:

- **Systematic** sampling (regular interval)
- **Cluster** sampling (sample groups, then measure everything in them)
- **Quasi-continuous** sampling (regular time series subsampling)
- **Blocked** designs (preview only — think “blocks as structure”)

All exercises below should run using the same objects from Part I:
- `df` (with `date`, `month`, `day_night`)
- `response_var`
- `pop_mean`, `pop_sd`
- your SRS sample `samp` and stratified sample `samp_strat` (if you created them)

> **Tip:** If you renamed objects in Part I, update the code below to match your names.

---

## Exercise 1 — Systematic sampling (every k-th observation)

**Idea:** sample at a fixed interval (e.g., every 48th record ≈ daily at 30-min resolution).

**Risk:** if the variable has strong cycles aligned with the interval, systematic sampling can be biased.

```{r systematic_sampling, echo=TRUE, eval=T}
# GOAL: Create a systematic sample and compare mean/sd to population.
# TODO: choose an interval k (try 48, 24, 96).
k <- 24

sys_samp <- df %>%
  # TODO: keep only non-missing response values
  filter(!is.na(.data[[response_var]])) %>%
  slice(seq(1, n(), by = k))

sys_mean <- mean(sys_samp[[response_var]], na.rm = TRUE)
sys_sd   <- sd(sys_samp[[response_var]], na.rm = TRUE)

tibble(
  strategy = c("Population", "Systematic"),
  mean = c(pop_mean, sys_mean),
  sd   = c(pop_sd,   sys_sd),
  n    = c(nrow(df), nrow(sys_samp))
)
```

**Prompt (3–4 sentences):** Did systematic sampling approximate the population mean/SD better or worse than SRS?  
What cycle (daily or seasonal) might be interacting with your chosen `k`?

> *Systematic sampling is approximating worse than SRS, because k is interacting with daily periodicity.*

---

## Exercise 2 — Cluster sampling (sample days, take all points within those days)

**Definition:** choose a set of clusters (here, **days**) at random, then include **all observations** in the chosen clusters.

This mimics EAES logistics: you may only be able to sample on certain days.

```{r cluster_sampling_days, echo=TRUE, eval=T}
# GOAL: Sample whole days as clusters, then estimate mean/sd.
# TODO: choose number of days to sample.
set.seed(480)

n_days <- 90

days <- df %>%
  distinct(date) %>%
  drop_na(date) %>%
  pull(date)

chosen_days <- sample(days, size = n_days, replace = FALSE)

cluster_samp <- df %>%
  filter(date %in% chosen_days) %>%
  filter(!is.na(.data[[response_var]]))

clust_mean <- mean(cluster_samp[[response_var]], na.rm = TRUE)
clust_sd   <- sd(cluster_samp[[response_var]], na.rm = TRUE)

tibble(
  strategy = c("Population", "Cluster (days)"),
  mean = c(pop_mean, clust_mean),
  sd   = c(pop_sd,   clust_sd),
  n    = c(nrow(df), nrow(cluster_samp))
)
```

**Prompt (3–4 sentences):** Why might cluster sampling have **higher variance** than SRS for the same number of measurements?  
What feature of time series data (hint: autocorrelation) is relevant here?

> *Because the observations within a given cluster are not independent (high correlation). Measurements taken close together exhibit strong autocorrelation.*

---

## Exercise 3 — Quasi-continuous sampling (fixed schedule time series)

**Definition:** sample regularly over time to create a *subsampled time series*.

This mimics continuous instrumentation that logs at a lower frequency (e.g., hourly instead of 30-min).

```{r quasi_continuous, echo=TRUE, eval=T}
# GOAL: Create a regular subsampled time series.
# TODO: choose a step size (every 2 records = hourly; every 4 = 2-hourly).
step <- 2

qc_samp <- df %>%
  filter(!is.na(.data[[response_var]])) %>%
  slice(seq(1, n(), by = step))

qc_mean <- mean(qc_samp[[response_var]], na.rm = TRUE)
qc_sd   <- sd(qc_samp[[response_var]], na.rm = TRUE)

tibble(
  strategy = c("Population", "Quasi-continuous"),
  mean = c(pop_mean, qc_mean),
  sd   = c(pop_sd,   qc_sd),
  n    = c(nrow(df), nrow(qc_samp))
)
```

### Visual check: does the subsampled series preserve structure?

```{r qc_plot, echo=TRUE, eval=T}
# TODO: plot BOTH population and qc_samp time series (thin lines) for a short window
# HINT: filter to one month to avoid overplotting.

df %>%
  filter(month == "Jan") %>%   # e.g., "Jul" if month is labeled
  ggplot(aes(x = date, y = .data[[response_var]])) +
  geom_line(alpha = 0.25) +
  theme_classic(base_size = 18) +
  labs(title = "Population time series (subset)", x = NULL, y = response_var)

qc_samp %>%
  filter(month == "Jan") %>%
  ggplot(aes(x = date, y = .data[[response_var]])) +
  geom_line(alpha = 0.6) +
  theme_classic(base_size = 18) +
  labs(title = "Quasi-continuous sample time series (subset)", x = NULL, y = response_var)
```

**Prompt (3–4 sentences):** Does quasi-continuous sampling preserve the **seasonal** pattern? The **daily** pattern?  
In an EAES context, when is quasi-continuous sampling preferable to sparse discrete sampling?

> *Yes, quasi-continuous sampling preserve the daily pattern and not the seasonal pattern. Quasi-continuous sampling is preferable when measuring variables driven by short-term cycles*

---

## Exercise 4 — Compare all strategies in one table (and interpret)

```{r compare_all, echo=TRUE, eval=T}
# GOAL: Assemble a comparison table of mean/sd error for each strategy.
# NOTE: This assumes you created objects: samp, samp_strat, sys_samp, cluster_samp, qc_samp.
# If your object names differ, update them here.

summ_stats <- function(dat, label) {
  tibble(
    strategy = label,
    n = nrow(dat),
    mean = mean(dat[[response_var]], na.rm = TRUE),
    sd   = sd(dat[[response_var]], na.rm = TRUE)
  )
}

bind_rows(
  tibble(strategy = "Population", n = nrow(df), mean = pop_mean, sd = pop_sd),
  summ_stats(samp, "SRS"),
  summ_stats(samp_strat, "Stratified (month)"),
  summ_stats(sys_samp, "Systematic"),
  summ_stats(cluster_samp, "Cluster (days)"),
  summ_stats(qc_samp, "Quasi-continuous")
) %>%
  mutate(
    mean_error = mean - pop_mean,
    sd_error   = sd - pop_sd
  )
```

**Prompt (5–6 sentences, graded):** Which strategy gave the best estimate of the population mean? Of the population SD?  
Explain *why* in terms of (i) seasonal/diurnal structure and (ii) dependence/autocorrelation.  
If you were designing a real EAES study with limited field days, what hybrid strategy would you propose (e.g., stratified + clustered)?

> *Stratified sampling by month provided the best estimates for both the population mean and SD. This strategy is better because it accounts for the strong seasonal pattern. Furthermore, by randomly selecting observations within those monthly strata, it mitigates the negative impacts of temporal autocorrelation. Systematic sampling performed the worst because its fixed interval likely aliased with the daily cycle. For a real field study with limited resources, I would propose a stratified-cluster hybrid design.*

---

## Blocked designs (preview only — do not implement yet)

A **blocked** design means sampling within blocks (time blocks like months, or spatial blocks like sites), then later treating block as structure in the model (often as a random effect).

You already approximated blocking by stratifying over `month`. Later, we will return to this idea when we fit models that include block structure explicitly.

# Submission + self-check

## Before you knit (Run All)
- [ ] Setup chunk runs without errors
- [ ] Data join happened (df_raw has both canopy + climate columns)
- [ ] All chunks run top-to-bottom
- [ ] No objects created only in the Console
- [ ] Interpretation answers are complete sentences

## After you knit
- [ ] Figures appear in the output
- [ ] Tables render and are readable
- [ ] Your selected x and y are clearly stated in the document

**Save, commit, push to Github.**
